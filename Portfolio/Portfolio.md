<h1>Persoonlijke Portfolio</h1>

Het persoonlijke portfolio voor de minor Data Science aan de Haagse Hogeschool

* <b>Naam</b>: Askin Sarikaya
* <b>Studentnummer</b>: 14121409
* <b>E-mail</b>: 14121409@student.hhs.nl


# Table of contents
1. [Introductie](#Introductie)
2. [Domain Knowledge](#Domein)
    1. [Jargon](#Jargon)
    2. [Literatuur](#Literatuur)    
3. [Courses](#Courses)
   1. [Datacamp Courses](#Datacamp)
   2. [Coursera Courses](#Coursera)
4. [Data Science Technieken](#Data)
    1. [Predictive Modeling](#Predictive)
    2. [Data Preparation](#Preparation)
    3. [Data Visualization](#Visualization)
    4. [Data Evaluation](#Evaluation)
    5. [Diagnostics](#Diagnostics)
5. [Communicatie](#Communicatie)
    1. [Presentaties](#Presentaties)
    2. [Paper](#Paper)
    3. [Reflectie](#Reflectie)



<h2>Introductie</h2>

Voor onze minor Data Science aan de Haagse Hogeschool hebben wij in een groep van 4 mensen een opdracht uitgevoerd namens het CBS.
Daarbij werd er getracht de top 10 categorieen, van gegeven data van het CBS, eruit te filteren.
Data die door het CBS beschikbaar is gesteld en tevens gebruikt is voor deze opdracht zijn de gestelde vragen per e-mail aan het CBS.
Omdat dit zodanig groot bleek te zijn, door het aantal categorieen waarin vragen kunnen worden gesteld meer dan 180 bleek, hebben wij ons als groep beperkt tot 2 datasets:

* Inkomen
* Bevolkingsgroep

Deze datasets hebben wij handmatig gelabeld en vervolgens geprobeerd om ons algoritme op los te laten. Daarbij hebben wij voornamelijk volgende modellen gebruikt:

* Multionomial Naive Bayes
* Complement Naive Bayes
* Logistic Regression

Dit waren ook tevens de modellen met het beste resultaat op onze skewed dataset. 

<h1>Domein</h1>

<h2>Jargon</h2>

* Naive Bayes = Predictive Model
* Skewed Dataset = Ongebalanceerde dataset
* Logistic Regression =
* Data Cleaning = Het verschonen en categoriseren van de dataset
* Machine Learning =
* Data Visualisatie = Het visualiseren van data
* Data Manipuleren = 
* Data Importeren =
* Pentesting = 
* OneVsRest

<h2>Literatuur</h2>
<h1>Courses</h1>
In dit hoofdstuk worden de benodigde en extra opdrachten die gemaakt zijn toegelicht.

<h2>Datacamp</h2>


Alle benodigde opdrachten beschreven in de wekelijkse agenda voor datacamp zijn voltooid. 
Hieronder wordt per course een korte stuk beschreven over de toegevoegde waarde van de course ten behoeve van mijn ontwikkeling.


* <b> Programmeren </b> (19100 XP)
1. (Course) [Introduction to Python](https://www.datacamp.com/courses/intro-to-python-for-data-science)
2. (Course) [Intermediate Python for Data Science](https://www.datacamp.com/courses/intro-to-python-for-data-science)
3. (Chapter) [Writing your own functions](https://www.datacamp.com/courses/python-data-science-toolbox-part-1)
4. (Chapter) [Default arguments, variable-length arguments and scope](https://www.datacamp.com/courses/python-data-science-toolbox-part-1)
5. (Course) [Python Data Science Toolbox (Part 2)](https://www.datacamp.com/courses/python-data-science-toolbox-part-2)

Het leren van het programmeren van Python heeft er tot bijgedragen dat ik op een basis-gevorderd niveau Python code kan begrijpen. 
Dit zorgde direct ervoor dat bij het lezen van de code gemaakt (vooral tutorials gevolgd op internet) door de programmeurs van de groep, de code niet vreemd overkwam bij mij op persoon.
Dit zorgde er tevens voor dat ik gericht ideeen/feedback kon geven passend op de haalbaarheid van de opdracht zonder dat dit buiten proporties kwam te liggen.

Boven alles heeft dit ervoor gezorgd dat ik kundiger met Linux en tegelijk met Python ben geworden. Dit heeft ervoor gezorgd dat ik in mijn eigen vakgebied (IT - Security) scripts kan begrijpen & schrijven als ik bijvoorbeeld iets aan het pentesten ben.
Deze relatie had ik voorheen niet kunnen leggen en dit is zodoende ook een grote positieve bijdrage in mijn carriere.


* <b> Importeren en Cleanen van Data </b> (8020 XP)
1. (Chapter) [Importing Data in Python (Part 1)](https://www.datacamp.com/courses/importing-data-in-python-part-1) & Mandatory
2. (Course) [Introduction and flat files](https://www.datacamp.com/courses/importing-data-in-python-part-1)
3. (Course) [Cleaning data in Python](https://www.datacamp.com/courses/cleaning-data-in-python)

* <b> Data Manipulatie </b> (2080 XP)
1. (Chapter) [Data ingestion & inspection](https://www.datacamp.com/courses/pandas-foundations)
2. (Chapter) [Exploratory Data Analysis](https://www.datacamp.com/courses/pandas-foundations)

* <b> Data Visualisatie </b> (3520 XP)
1. (Chapter) [Plotting 2D Arrays](https://www.datacamp.com/courses/introduction-to-data-visualization-with-python)
2. (Chapter) [Statistical plots with Seaborn](https://www.datacamp.com/courses/introduction-to-data-visualization-with-python)
3. (Chapter) [Customizing Plots](https://www.datacamp.com/courses/introduction-to-data-visualization-with-python)

* <b> Waarschijnlijkheid & Statistiek </b> (4350 XP)
1. (Course) [Statistical Thinking in Python (Part 1)](https://www.datacamp.com/courses/statistical-thinking-in-python-part-1)

* <b> Machine Learning </b> (4300 XP)
1. (Course) [Supervised Learning with scikit-learn](https://www.datacamp.com/courses/supervised-learning-with-scikit-learn)


<h2>Coursera</h2>


Voor Coursera zijn de weken 1,2,3 en 6 afgerond. De bijbehorende opdrachten zijn daarbij niet gemaakt. 
Alle quiz onderdelen zijn met een voldoende afgerond.

![Coursera](/Portfolio/Courses/Timeline%20Coursera.png)


De video's van Coursera en de bijbehorende quiz hebben bijgedragen tot een betere kennis op het gebied van machine learning. 
Bij Datacamp lag de focus meer op het toepassen en programmeren. Bij Coursera werd de achterliggende gedachte, formules etc. het gehele concept uitgelegd over machine learning dus ook toepassen.
Dat ik de stof begreep getuigd ook van mijn voldoende op de toets. Tevens zorgde het bestuderen van Coursera ervoor dat ik nieuwe ideeen opdeed en dit zorgde er direct voor dat bepaalde handelingen veel makkelijker konden uitgevoerd. Immers, wij begrepen het concept van machine learning veel beter nu.


<h1>Data</h1>
In dit hoofdstuk worden de stukjes code die ik heb gemaakt voor het project uitgelegd.


<h2>Predictive</h2>

Voor onze dataset bleek het beste dat er gekozen werd voor een supervised manier van leren. Hierbij zijn wij voorbarig begonnen met allerlei modellen te testen op onze data alvorens wij uberhaupt iets wisten over data science.
Na wat onderzoek en veel verder in het project, zijn wij tot conclusie gekomen dat de volgende 3 modellen het best werken voor onze dataset.

* Multinomial Naive Bayes
* Complement Naive Bayes
* Logistic Regression

Deze 3 predictive models hebben wij vervolgens ook toegepast op onze dataset. Daarbij behoort onder andere de volgende code:



```python

class MultiClassifier(BaseModel):
    def __init__(self, trainDF):
        super().__init__()
        prePro = PreProcessor()
        self.pf = PlotFunctions()
        self.trainDF = trainDF
        self.X_train, self.X_test, self.y_train, self.y_test = \
            prePro.split_train_test(trainDF['cleaned_sentence'], trainDF['classification'], 0.4)
        self.X_test, self.X_cross, self.y_test, self.y_cross = \
            prePro.split_train_test(self.X_test, self.y_test, 0.5)
        
        self.all_scores = list()
        self.models = {
            'MultinomialNB': naive_bayes.MultinomialNB(alpha=0.767, class_prior=None, fit_prior=True),
            'ComplementNB': naive_bayes.ComplementNB(alpha=0.767, class_prior=None, fit_prior=True),
            'LogisticRegression': linear_model.LogisticRegression(solver='lbfgs')
        }
        
```

Dit stukje code is het begin van het testen van ons model. Het bevat onder andere een preprocessor(voor het cleanen van de data). De code test op de 3 predictive modellen hierboven genoemd, ook split de code de dataset in een training / test / cross set.
Dit was in het begin niet voldoende, alleen testen op modellen bleek niet de juiste resultaat weer te geven op onze dataset. Na uitgebreid onderzoek en advies zijn wij terecht gekomen op zogeheten word embeddings.
Deze word embeddings geven tekst een bepaalde waarde in nummers. Hierdoor kan het model beter begrijpen welke woordcombinatie zwaarder weegt dan het ander. In dit geval zou het gebruikt worden om een bepaalde vraag te herkennen.
Zo hebben wij gekeken hoe een vraag zin wordt opgebouwd in het Nederlands en dit als input gegeven aan het model.

Omdat Word Embeddings meerdere modellen kent heb ik er twee gekozen om dit te gebruiken voor ons model. Dit waren de modellen TF - IDF Ngram en Count Vectors.


* TF - IDF Ngram = Hoe belangrijk een woord is in een document of in een collectie van documenten
```python
    def tfidf_ngram(self, features):
        tfidf_vect_ngram = TfidfVectorizer(
            analyzer='word', token_pattern=r'\w{1,}', ngram_range=(2, 5), max_features=features)
        tfidf_vect_ngram.fit(self.trainDF['cleaned_sentence'])
        xtrain_tfidf = tfidf_vect_ngram.transform(self.X_train)
        xvalid_tfidf = tfidf_vect_ngram.transform(self.X_test)
        xcross_tfidf = tfidf_vect_ngram.transform(self.X_cross)

        for model_name, model in self.models.items():
            mc_model = multiclass.OneVsRestClassifier(model)
            classifier = mc_model.fit(xtrain_tfidf, self.y_train)

            # Training predictions
            self.check_model(classifier, xtrain_tfidf, self.y_train, model_name, features, 'tfidf_ngram', 'training')

            # Test predictions
            self.check_model(classifier, xvalid_tfidf, self.y_test, model_name, features, 'tfidf_ngram', 'test')

            # Cross Validation predictions
            self.check_model(classifier, xcross_tfidf, self.y_cross, model_name, features, 'tfidf_ngram', 'cross')
```
<i>Predictive model met TFIDF - ngram op onze dataset</i>

Dit model traint op de aantal features en daarbij is gekozen voor een OneVsRest classifier.
Deze strategie zorgt ervoor dat je een classifier fit per class. Voor elke classifier is de class gefit tegen alle andere classes.
Het is gebruikelijk om voor de OneVsRest classifier te kiezen bij een Multi-class classification.
Tot slot split dit stukje code de data in Training, Test en een Cross Validation set.

Voor het model van de count_vectors is hetzelfde principe toegepast. Te zien hieronder:

* Count Vectors.
```python
    def count_vectors(self, features):
        count_vect = CountVectorizer(analyzer='word', token_pattern=r'\w{1,}', max_df=1.0, max_features=features)
        count_vect.fit(self.trainDF['cleaned_sentence'])
        xtrain_count = count_vect.transform(self.X_train)
        xvalid_count = count_vect.transform(self.X_test)
        xcross_count = count_vect.transform(self.X_cross)

        for model_name, model in self.models.items():
            mc_model = multiclass.OneVsRestClassifier(model)
            classifier = mc_model.fit(xtrain_count, self.y_train)

            # Training predictions
            self.check_model(classifier, xtrain_count, self.y_train, model_name, features, 'count_vectors', 'training')

            # Test predictions
            self.check_model(classifier, xvalid_count, self.y_test, model_name, features, 'count_vectors', 'test')

            # Cross Validation predictions
            self.check_model(classifier, xcross_count, self.y_cross, model_name, features, 'count_vectors', 'cross')

```
<i>Predictive model met count vectors op onze dataset.</i>

De toegevoegde waarde van mijn code op het project was het feit dat er ten eerste predictive model(len) zijn toegepast. Vervolgens zijn deze modellen ook met bovengenoemde vormen van word embedding toegepast op onze data.
Daaruit is gebleken welk model het beste was voor onze dataset. Uiteindelijk heeft het ervoor gezorgd dat er een vergelijking is gemaakt tussen allerlei modellen, en daaruit is het beste model gekozen.
Ook omdat elk model getest is tegenover Logistic Regression, Multinomial NB & Complement NB, kan de conclusie getrokken worden welk predictive model + word embeddings het best werken op de dataset.

Tot slot is gebleken dat de predictive model Logistic Regression met count vectors het beste model bleek te zijn voor ons dataset.


<i>Uiteindelijke resultaat van F1-score bij cross validatie set op datasets met verschillende verhoudingen van classificatie 3 vs de rest.</i>
![Test](/Portfolio/Courses/Screenshot%202019-01-11%20at%2020.06.46.png)


* Ngram = Model over de relatie tussen woorden. Daarbij creeert het bijvoorbeeld, Unigram(1 woord), Bigram(2 woorden), Trigram (3 woorden) etc.
In het geval van een bigram kunnen we meegeven dat 2 bepaalde woorden bij elkaar een bepaalde opbouw van een zin aangeven bijvoorbeeld.


<h2>Preparation</h2>

Voor het project heb ik op het gebied van Data preparation een stukje datacleaning toegepast. 

Dit kan uitgevoerd worden in verschillende manieren, ik heb er twee toegepast op de dataset:

* Aggregeren van Data - Het in een leesbare tabel zetten van verkregen e-mail data(<i>uitgevoerd door mij op de dataset </i>)

Hiervoor heb ik gebruik gemaakt van de package pandas in Python. Dit is geleerd op de courses van datacamp:

```python
import pandas as pd


# Data laden van een .csv file
data = pd.DataFrame.from_csv('cbs.data')
# Converteren van data van een string naar tijd
data['date'] = data['date'].apply(dateutil.parser.parse, dayfirst=True)
```

Dit was een stukje die ik had toegepast om in ieder geval de data van CBS e-mails in een datum formaat te zetten zodat het duidelijk was welke email op welke datum is verstuurd.

&


* Het invoeren van data waar cellen leeg staan. Meestal wordt er door een script gekeken waar data leegstaat.
Alle data die het model als <1 herkent wordt vervangen met een 0. Soms wordt het vervangen door een NaN = Not a Number.

Bij het zogeheten cleanen en voorbereiden van data heb ik een aantal tutorials gevolgd waarbij naar voren kwam hoe men leegstaande cellen zo goed mogelijk kon aanpakken. 
Daarbij is door mij de volgende code gehanteerd:

```python
# Lijst van alle leegstaande waarde
lege_waarden = ["n/a", "na", "--"]
df = pd.read_csv("cbs.csv", na_values = lege_waarden)
```

Zoals in de comment staat, zorgt dit stukje code ervoor dat het een lijst maakt van de dataset waarbij alle data die leegstaat wordt geinventariseerd.
Vervolgens heb ik gekozen om leegstaande vakken te vervangen door een nummer met de volgende code:

```python
# Leegstaande waarde veranderen door een nummer
df['cbs].fillna(125, inplace=True)
```


De toegevoegde waarde van het cleanen van data is dat er vooral een overzicht komt van de bruikbare data. Ook kan het ervoor zorgen dat men een verborgen patroon herkent in de data die voorheen niet gezien kon worden.
Tot slot kan het ervoor zorgen dat men bij het debuggen uren werk minder hoeft te doen wanneer je een diagnose / analyse gaat uitvoeren op je data.


Tevens heb ik samen met mijn collega's data gelabeld in 4 classificaties voor het predictive model. De code hiervoor en mijn aandeel staat beschreven in het kopje Predictive Models




<h2>Visualization</h2>

Confusion Matrix Visualisatie 

```python
def create_confusion_matrix(self, valid_y, predictions_valid, model_name):
    # Compute confusion matrix
    cnf_matrix = confusion_matrix(valid_y, predictions_valid)
    np.set_printoptions(precision=2)
    # Plot non-normalized confusion matrix
    plt.figure()
    class_names = ['Beschikbaarheidsvraag', 'Verduidelijkingsvraag', 'Niet relevant', 'Relevante query vraag']
    self.plot_confusion_matrix(cnf_matrix,
                               classes=class_names,
                               title=model_name + ' Confusion matrix, without normalization')

    # Plot normalized confusion matrix
    plt.figure()
    self.plot_confusion_matrix(cnf_matrix, classes=class_names,
                               normalize=True,
                               title=model_name + ' Normalized confusion matrix')

    plt.show()
```

Het idee van het uitvoeren van een confusion matrix op de test set is om te kijken hoe het model een voorspelling uitvoert ten opzichte van onze voorspelling.
Zo kon er vervolgens bepaald worden op welke punten het model niet een goede voorspelling deed, of dat er sprake was van een overfit of iets dergelijks.
Ook kon er een diagnose worden gedaan van de machine learning model in vorm van een Error Analyse.

In de gemaakte code kan men 2 soorten modellen van de confusion matrix onderscheiden:

* Normalized Confusion Matrix
* Non - Normalized Confusion Matrix


![test1](/Portfolio/Courses/Test-Normalized.png)
![test2](/Portfolio/Courses/Test-Not-Normalized.png)

Resultaten uit de Confusion Matrix met bovenstaande code.
Om het bovenstaande te visualiseren heb ik samengewerkt met Timo Frionnet om de code te realiseren. 
Voor de error analyse gebaseerd op de gemaakte confusion matrix verwijs ik naar het kopje Diagnostics



<h2>Data collection</h2>
Dit is niet relevant geweest voor ons onderzoek, aangezien alle beschikbare data door het CBS is vrijgegeven. Wellicht kan het stukje labelen vallen onder het kopje "Data Collection".
Hierbij hebben mijn collega's en ik de relevante datasets doorlopen en gelabeld als een 1,2,3 of 4 classificatie.


```python
Multi-class classification | Vier classificaties
De multi-class classification bestaat uit de volgende vier classificaties:
•	Niet-relevante beschikbaarheidsvraag: classificatie 1
Beschikbaarheidsvragen zijn niet-relevante vragen waarin een verzoek wordt gediend om informatie te verkrijgen over:
o	wanneer nieuwe cijfers gepubliceerd en/of geüpdatet zullen worden.
•	Niet-relevante verduidelijkingsvraag: classificatie 2
Verduidelijkingsvragen zijn niet-relevante vragen waarin een verzoek wordt gediend om informatie te verkrijgen over één of meerdere onderdelen:
o	totstandkoming van specifieke cijfers in publicaties en/of Statline.
o	definities van de gehanteerde termen in publicaties en/of Statline.
•	Niet-relevante zinnen: classificatie 3
Niet-relevante zinnen zijn zinnen waarin meningsuitingen over maatschappelijke onderwerpen worden gegeven, afsluitingen van e-mails en introductie van e-mails.

•	Relevante query vraag: classificatie 4
Query vragen zijn relevante vragen waarin een verzoek wordt gediend om informatie te verkrijgen over één of meerdere onderdelen:
o	specifieke cijfers van een onderwerp wat binnen een thema van het CBS valt. 
o	concrete gegevens van een onderwerp wat binnen een thema van het CBS valt. Denk hierbij aan inkomen wat onder CBS-categorie ‘Beroepsbevolking’ valt.
Bij een query vraag wordt de context van de vraag tevens als query vraag geclassificeerd, zodat de input van een query als volledig wordt beschouwd. 
```




<h2>Evaluation</h2>

```python

    def count_vectors(self, cvalue):
        count_vect = CountVectorizer(analyzer='word', token_pattern=r'\w{1,}', max_df=1.0, max_features=1500)
        count_vect.fit(self.trainDF['cleaned_sentence'])
        xtrain_count = count_vect.transform(self.X_train)
        xvalid_count = count_vect.transform(self.X_test)
        xcross_count = count_vect.transform(self.X_cross)

        for model_name, model in self.models.items():
            model.C = cvalue
            mc_model = multiclass.OneVsRestClassifier(model)
            classifier = mc_model.fit(xtrain_count, self.y_train)

            # Training predictions
            self.check_model(classifier, xtrain_count, self.y_train, model_name, features, 'count_vectors', 'training')

            # Test predictions
            self.check_model(classifier, xvalid_count, self.y_test, model_name, features, 'count_vectors', 'test')

            # Cross Validation predictions
            self.check_model(classifier, xcross_count, self.y_cross, model_name, features, 'count_vectors', 'cross')
```
E.v.t Uitleg toegevoegde waarde en breakdown code
Gehanteerde Range, Logistic Regression:

```python
    def get_and_print_all_scores(self):
        print('Running for count_vectors')
        for i in range(-5, 5):
            self.count_vectors(10**i)
            self.tfidf_words(i)
            self.tfidf_ngram(i)
            self.tfidf_char(i)
```


Voor het bepalen of de data overfit of underfit is, heb ik de volgende functie gemaakt. Deze functie is een soortgelijke functie gebruikt bij het predictive modellen.

Daaruit is het volgende gekomen:

![LG](/Portfolio/Courses/Screenshot%202019-01-11%20at%2013.53.33.png)
 
In dit model is te zien data voor de -1 Range underfit is en boven de 1 overfit te zien aan de testdata.


<h2>Diagnostics</h2>

Aanvullend op de confusion matrix visualisatie, heb ik een diagnose uitgevoerd op dit model en de test dataset 
Dit is te vinden onder het stukje: Predictive Modeling, Error Analyse.

Hierbij bestudeer ik het model en maak ik een vergelijking met het door de onderzoekers voorspelde classificatie in tegenstelling tot wat het model voorspelt.




<h1>Communicatie</h1>
<h2>Presentaties</h2>

Presentaties zijn altijd gezamenlijk gemaakt met de inbreng van de groep. Omdat ik, zoals aangegeven, meer in de kant van het onderzoeken was, presenteerde ik ook veel meer. 
In totaal heb ik 10 keer gepresenteerd waarvan 2 keer alleen. 

Presentaties per week:

* [Week 1](/Presentaties/2018.08.31-intro.pptx)
* [Week 2](/Presentaties/2018.09.07%20Presentatie.pptx) 
* [Week 3](/Presentaties/2018.09.10%20CBS%20Presentatie.pptx)
* [Week 4](/Presentaties/2018.09.14%20Presentatie.pptx)
* [Week 5](/Presentaties/2018.09.21%20CBS%20Presentatie.pptx)
* [Week 6](/Presentaties/2018.09.28%20CBS%20Presentatie.pptx)
* [Week 7](/Presentaties/2018.10.05%20CBS%20Presentatie.pptx)
* [Week 8](/Presentaties/2018.10.12%20CBS%20Presentatie.pptx)
* [Week 9](/Presentaties/2018.10.19%20CBS%20Presentatie.pptx)
* [Week 10](/Presentaties/2018.11.02%20CBS%20Presentatie.pptx)
* [Week 11](/Presentaties/2018.11.09%20CBS%20Presentatie.pptx)
* [Week 12](/Presentaties/2018.11.16%20CBS%20Presentatie.pptx)
* [Week 13](/Presentaties/2018.11.30%20CBS%20Presentatie.pptx)
* [Week 14](/Presentaties/2018.12.07%20CBS%20Presentatie.pptx)
* [Week 15](/Presentaties/2018.12.17%20CBS%20Presentatie%20%5BAutosaved%5D.pptx)
* [Week 16](/Presentaties/2018.12.21%20CBS%20Presentatie.pptx)


<h2>Paper</h2>
De paper is een gezamenlijke bijdrage van de gehele groep. 
Omdat ik geen fervente coder ben, heb ik samen met mijn collega Seyma Irilmazbilek vooral gericht tot de taak onderzoeken en delen van kennis(o.a. aanpak, ideeen etc.) aan ons groepsgenoten binnen dit blok. 
Zodoende was de paper meer mijn domein. 

Zo heb ik het volgende uitgevoerd binnen de paper:


* Related Work
* Gedeelte Methode
* Gedeelte Aanpak
* Bronnen uitzoeken relevant voor ons opdracht, uitdragen en citeren in het verslag (Graag verwijs ik naar kopje literatuur)
* (Code)Error Analyse samen met Timo Frionnet 
* Conclusie & Discussie
* APA Verslag, Figuren, Vergelijkingen, Bronnen
* Layout

Het gehele rapport zal ook apart worden ingeleverd. I.v.m. de vertrouwelijkheid van data zal ik niet naar het rapport verwijzen in dit portfolio.


<h2>Scrum</h2>
Omdat ik niet de grootste fan van scrum ben, heb ik dit ook nauwelijks gebruikt. Voor een breakdown van Scrum per persoon, refereer ik naar de [Scrumwise](https://www.scrumwise.com/scrum/#/people/project/kb74-2018-cbs) pagina van de CBS projectgroep.

Taken en activiteiten beknopt:  

* Domein Studie: Dit houdt in dat ik onderzoek deed naar alle gerelateerde onderzoeken op dit gebied. Maar ook naar bepaalde gehanteerde methodieken etc. Dit heb ik vervolgens geinventariseerd en uitgedragen binnen het project groep.
* Het onderzoeksplan voor ons opdracht opstellen en presenteren aan het CBS
* Linear Classifier Methode Onderzoeken & Beschrijven
* Logistic Regression Methode Onderzoeken & Beschrijven
* Word Level TF - IDF Methode Onderzoeken & Beschrijven
* Linear Regression & TF - IDF & Naive Bayes Voordelen, Nadelen omschrijven per type model
* Language Models Onderzoeken
* Formule uitleggen Extreme Gradient Boosting & Linear Regression
* Onderzoek Deep Learning Models
* Related Work - Bronnen onderzoeken gerelateerd aan ons project
* Research Paper
* POS Tagging > Text Classificatie methoden en modellen onderzoek
* Multinomial Naive Bayes > Onderzoek
* Text Classificatie methoden en modellen onderzoeken & uitschrijven
* Datasets labelen in 4 classificaties
* Confusion Matrixes maken van gelabelde datasets
* Error Analyse over Training - Test - Cross set

<h2> Reflectie </h2>
